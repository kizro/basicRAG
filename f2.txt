Jailbreaking Black Box Large Language Models in
Twenty Queries

arXiv:2310.08419v4 [cs.LG] 18 Jul 2024

Patrick Chao, Alexander Robey,
Edgar Dobriban, Hamed Hassani, George J. Pappas, Eric Wong
University of Pennsylvania
Originally submitted: October 12, 2023
Last updated: July 18, 2024

Abstract
There is growing interest in ensuring that large language models (LLMs) align
with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The
identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt
Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR—which is inspired by social
engineering attacks—uses an attacker LLM to automatically generate jailbreaks
for a separate targeted LLM without human intervention. In this way, the attacker
LLM iteratively queries the target LLM to update and refine a candidate jailbreak.
Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak,
which is orders of magnitude more efficient than existing algorithms. PAIR also
achieves competitive jailbreaking success rates and transferability on open and
closed-source LLMs, including GPT-3.5/4, Vicuna, and Gemini.

1

Introduction

Although still at its infancy, the field of study surrounding large language models (LLMs) has shown
significant promise in advancing numerous fields, including code generation [1], business analytics [2], and medicine [3]. The strong performance of LLMs is largely due to the fact that they are
trained on vast text corpora, which in turn facilitates the generation of realistic text that pertains to
a diverse set of topics [4, 5]. However, one drawback of this approach is that these corpora often
contain toxic or objectionable content, which, when propagated by an LLM trained on this data, has
the propensity to cause harm [6]. For this reason, it has become common practice to implement
various mechanisms that “align” the content generated by LLMs with human values [7–10].
Despite these efforts, two classes of so-called jailbreaking attacks have recently been shown to bypass LLM alignment guardrails [12–15], leading to concerns that LLMs may not yet be suited for
wide-scale deployment in safety-critical domains. The first class of prompt-level jailbreaks comprises social-engineering-based, semantically meaningful prompts which elicit objectionable content from LLMs. While effective (see, e.g., [16, 17]), this technique requires creativity, manual
dataset curation, and customized human feedback, leading to considerable human time and resource
investments. The second class of token-level jailbreaks involves optimizing the set of tokens passed
as input to a targeted LLM [18, 19]. While highly effective [11, 20], such attacks require hundreds
of thousands of queries to the target model and are often uninterpretable to humans.
Before LLMs can be trusted in safety-critical domains, it is essential that the AI safety community
design realistic stress tests that overcome the drawbacks of both prompt- and token-level jailbreaks.
Preprint. Under review.

Figure 1: Prompt- vs. token-level jailbreaks. (Top) A token-level jailbreak generated by GCG [11]. (Bottom) A promptlevel jailbreak generated by PAIR.

Figure 2: PAIR schematic. PAIR pits an attacker
and target LLM against one another; the attacker’s
goal is to generate adversarial prompts that jailbreak
the target model in as few queries as possible.

To this end, in this paper we aim to strike a balance between the labor-intensive and non-scalable
prompt-level jailbreaks with the uninterpretable and query-inefficient token-level jailbreaks. Our
approach—which we call Prompt Automatic Iterative Refinement (PAIR)—is designed to systematically and fully automate prompt-level jailbreaks without a human in the loop. At a high level,
PAIR pits two black-box LLMs—which we call the attacker and the target—against one another, in
that the attacker is instructed to discover candidate prompts which jailbreak the target (see Fig. 2).
Our results indicate that PAIR efficiently discovers prompt-level jailbreaks within twenty queries,
which represents a more than 250-fold improvement over existing attacks such as GCG [11]. Moreover, the attacks generated by PAIR display strong transferability to other LLMs, which is largely
attributable to the human-interpretable nature of its attacks.
Contributions. We propose a new algorithm—which we term PAIR—for efficiently and effectively
generating interpretable, prompt-level jailbreaks for black-box LLMs.
• Efficiency. PAIR is parallelizable, runs on CPU or GPU, and uses orders of magnitudes fewer
queries than existing jailbreaks. When attacking Vicuna-17B, on average PAIR finds successful
jailbreaks in 34 (wall-clock) seconds using 366MB of CPU memory at a cost of less than $0.03.
• Effectiveness. PAIR jailbreaks open- and closed-source LLMs; it achieves a jailbreak percentage of 50% for GPT-3.5/4, 88% for Vicuna-13B, and 73% for Gemini-Pro. To the best of our
knowledge, PAIR is the first automated jailbreak that has been shown to jailbreak Gemini-Pro.
• Interpretability. PAIR generates prompt-level semantic jailbreaks that are interpretable to humans and includes interpretable, chain-of-thought improvement assessments. We also find that
PAIR jailbreaks are often more transferrable to other LLMs than jailbreaks generated by GCG.

2

Preliminaries

We focus on prompt-level jailbreaks, wherein the goal is to craft semantic, human-interpretable
prompts that fool a targeted LLM into outputting objectionable content. To make this more precise,
assume that we have query access to a black box target LLM, which we denote as T . Given a prompt
P = x1:n , where x1:n := (x1 , . . . , xn ) is the tokenization of P , a response R = xn+1:n+L containing L tokens (xn+1 , . . . , xn+L ) is generated from T by sampling from the following distribution:1
qT∗ (xn+1:n+L |x1:n ) :=

L
Y

qT (xn+i |x1:n+i−1 )

(1)

i=1

Thus, if we let V denote the vocabulary (i.e., the set of all tokens), then qT : V ∗ → ∆(V) represents a
mapping from a list of tokens of arbitrary length (in the set V ∗ ) to the set of probability distributions
∆(V) over tokens. To simplify this notation, we write R ∼ qT (P ) to denote sampling a response
R from qT given a prompt P , with the understanding that both P and R are tokenized as x1:n
and xn+1:n+L respectively when passed to the LLM. Given this notation, our objective is to find
a prompt P that elicits a response R containing objectionable content from T . More formally, we
1

PAIR is a black-box attack; it only requires sampling access, rather than full access, to qT .

2

seek a solution to the following problem:
find P s.t. JUDGE(P, R) = 1 where R ∼ qT (P )
∗

(2)

∗

where JUDGE : V × V → {0, 1} is a binary-valued function that determines whether a given
prompt-response pair (P, R) is jailbroken2 . While straightforward to pose, in practice, determining which pairs (P, R) constitute a jailbreak tends to be a significant challenge [21]. To this end,
throughout the paper we assume that each jailbreak is characterized by an objective O, which describes the toxic content that the attacker seeks to elicit from the target (e.g., “tell me how to build a
bomb”). The objective O informs the generation and evaluation of prompts, ensuring that generated
jailbreaks are contextually relevant and aligned with the specific malicious intent being simulated.
Related work: Prompt-based jailbreaks. When training LLMs, it is common practice to use human annotators to flag prompts that generate objectionable content. However, involving humans in
the training loop limits scalability and exposes human annotators to large corpora of toxic, harmful,
and biased text [16, 17, 22, 23]. While there have been efforts to automate the generation of promptlevel jailbreaks, these methods require prompt engineering [24], manually-generated test cases [25],
or retraining large generative models on objectionable text [26], all of which hinders the widespread
deployment of these techniques. To this end, there is a need for new automated jailbreaking tools
that are scalable, broadly applicable, and do not require human intervention.

3

Generating prompt-level jailbreaks with PAIR

To bridge the gap between existing interpretable, yet inefficient prompt-level attacks and automated, yet non-interpretable token-level attacks, we propose Prompt Automatic Iterative Refinement
(PAIR), a new method for fully automated discovery of prompt-level jailbreaks. Our approach is
rooted in the idea that two LLMs—namely, a target T and an attacker A—can collaboratively
and creatively identify prompts that are likely to jailbreak the target model. Notably, because
we assume that both LLMs are black box, the attacker and target can be instantiated with any
LLMs with publicly-available query access. This contrasts with the majority of token-level attacks
(e.g., [11, 27]), which require white-box access to the target LLM, resulting in query inefficiency
and limited applicability. In full generality, PAIR consists of four key steps:
1. Attack generation: We design targeted, yet flexible system prompts which direct the attacker
A to generate a candidate prompt P that jailbreak the target model.
2. Target response: The prompt P is inputted into the target T , resulting in a response R.
3. Jailbreak scoring: The prompt P and response R are evaluated by JUDGE to provide a score S.
4. Iterative refinement: If S = 0, i.e., the pair (P, R) was classified as not constituting a jailbreak, P , R, and S are passed back to the attacker, which generates a new prompt.
As we show in §4, this procedure critically relies on the back-and-forth conversational interaction
between the attacker and the target, wherein the attacker A seeks a prompt that fools the target T
into generating a response R, and then R is fed back into A to generate a stronger candidate prompt.
3.1

Implementing the attacker LLM

Fundamental to effectively and efficiently generating PAIR jailbreaks is the choice and implementation of the attacker model A, which involves three key design considerations: the design of the
attacker’s system prompt, the use of the chat history, and an iterative assessment of improvement.
Attacker’s system prompt. Given the conversational nature of the previously described steps, the
efficacy of this attack critically depends on the design of the attacker’s system prompt. To this end,
we carefully design three distinct system prompts templates, all of which instructs the LLM to output
a specific kind of objectionable content. Following [28], each system prompt template is based on
one of three criteria: logical appeal, authority endorsement, and role-playing. Within each system
prompt, we also provide several examples specifying the response format, possible responses and
improvements, and explanations motivating why these attacks may be successful (see App. F.2 for
the full system prompts). As we show in §4, these criteria can result in vastly different jailbreaks.
2
In this setting, we choose the function JUDGE to receive both the prompt and the response as input to allow
the judge to inspect the candidate adversarial prompt for context. It is also valid to choose a JUDGE function
that only depends on the response R.

3

Table 1: JUDGE classifiers. Comparison of JUDGE functions across 100 prompts and responses. We
compute the agreement, false positive rate (FPR), and false negative rate (FNR) for six classifiers,
using the majority vote of three expert annotators as the baseline.
JUDGE function
Baseline

Metric

GPT-4

GPT-4-Turbo

GCG

BERT

TDC

Llama Guard

Human Majority

Agreement (↑)
FPR (↓)
FNR (↓)

88%
16%
7%

74%